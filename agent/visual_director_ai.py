"""
AI Visual Director

This module orchestrates AI-based visual generation using Gemini API
for image generation to create highly relevant and context-aware visuals.
"""

import os
import logging
import json
from typing import Dict, Any, Optional, List

from .images.generate_ai import run as generate_ai_images_main
from .ai_prompt_crafter import create_visual_prompt
from .utils import get_audio_duration
from .visual_director.visual_timing_alignment import add_static_shots


def run(run_dir: str, transcript: Dict[str, Any], full_script: str, creative_brief: Dict[str, Any], 
        logger, ai_config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    Main AI visual director function that orchestrates the complete AI visual generation process.

    Args:
        run_dir: Directory for the current run
        transcript: Transcript data with word timings
        full_script: The complete narration script for context
        creative_brief: Creative brief with story information
        logger: Decision logger instance
        ai_config: Configuration for AI visual generation

    Returns:
        Dictionary containing AI-generated visual results or None if failed.
    """
    try:
        logging.info("ü§ñ Starting AI Visual Director process with Gemini API...")
        
        # Create visuals directory if it doesn't exist
        visuals_dir = os.path.join(run_dir, 'visuals')
        os.makedirs(visuals_dir, exist_ok=True)
        
        # Load visual story plan
        story_plan_path = os.path.join(run_dir, 'visual_story_plan.json')
        if not os.path.exists(story_plan_path):
            logging.error("Visual story plan not found - required for AI visual generation")
            return None
        
        with open(story_plan_path, 'r', encoding='utf-8') as f:
            visual_story_plan = json.load(f)
        
        logging.info("üìã Loaded visual story plan for AI generation")
        
        segments = visual_story_plan.get('visual_segments', [])
        
        enhanced_timeline = []
        visual_map = {}
        
        # Collect all prompts for batch generation
        prompts_to_generate = []
        segment_info = []
        
        for i, segment in enumerate(segments):
            cue_id = f"visual_ai_{i:02d}"
            primary_term = segment.get('primary_search_term', f'segment_{i}')
            logging.info(f"\nüß† Processing {cue_id}: {primary_term}")
            
            # 1. Create a detailed, context-aware prompt
            positive_prompt, negative_prompt = create_visual_prompt(segment, full_script)
            
            logger.log_decision(
                step=f"prompt_generation_{cue_id}",
                decision="Generated detailed prompt for AI visual generation",
                reasoning=f"Based on narrative context, emotion: '{segment.get('emotional_tone')}', and intent.",
                metadata={'positive_prompt': positive_prompt, 'negative_prompt': negative_prompt}
            )
            
            # Add to batch generation list
            prompts_to_generate.append(positive_prompt)
            segment_info.append({
                'cue_id': cue_id,
                'segment': segment,
                'primary_term': primary_term,
                'positive_prompt': positive_prompt,
                'negative_prompt': negative_prompt
            })
        
        # 2. Generate all images using Gemini API
        logging.info(f"üé® Generating {len(prompts_to_generate)} images with Gemini API...")
        generated_images = generate_ai_images_main(prompts_to_generate, visuals_dir)
        
        if not generated_images:
            logging.error("‚ùå No images were generated by Gemini API")
            return None
        
        logging.info(f"‚úÖ Generated {len(generated_images)} images")
        
        # 3. Process each generated image and create timeline entries
        for i, (image_path, segment_data) in enumerate(zip(generated_images, segment_info)):
            cue_id = segment_data['cue_id']
            segment = segment_data['segment']
            primary_term = segment_data['primary_term']
            
            if not image_path or not os.path.exists(image_path):
                logging.error(f"CRITICAL: No image generated for {cue_id}")
                continue
            
            # For now, we're using static images (no video animation)
            local_visual_path = image_path
            visual_map[cue_id] = local_visual_path
            
            # Create timeline entry
            start_time = segment.get('start_time', i * 4.0) # Fallback timing
            end_time = segment.get('end_time', (i + 1) * 4.0)
            
            enhanced_entry = {
                'cue_id': cue_id,
                'start_time': start_time,
                'end_time': end_time,
                'trigger_keyword': primary_term,
                'visual_type': segment.get('visual_type', 'Abstract Concept'),
                'visual_file': local_visual_path,
                'asset_info': {
                    'source': 'ai_generated',
                    'type': 'image',  # Using images for now
                    'licence': 'proprietary',
                    'attribution': 'AI Generated - Gemini API'
                },
                'generation_metadata': {
                    'model': 'gemini-2.0-flash-preview-image-generation',
                    'prompt': segment_data['positive_prompt']
                }
            }
            enhanced_timeline.append(enhanced_entry)
            
            logging.info(f"‚úÖ Successfully processed visual for {cue_id}: {os.path.basename(local_visual_path)}")

        # Add opening webpage video and closing logo to timeline
        webpage_video_path = os.path.join(run_dir, 'webpage_capture.mp4')
        logo_path = "E:\\Dev\\AutoSocialMedia\\assets\\company_logo_closing.mp4"
        
        # Check if webpage video exists
        if not os.path.exists(webpage_video_path):
            logging.error(f"Webpage capture video not found: {webpage_video_path}")
            return None
        
        # Get total audio duration
        audio_path = os.path.join(run_dir, 'voice.mp3')
        total_audio_duration = get_audio_duration(audio_path) if os.path.exists(audio_path) else 30.0
            
        # Add static shots to timeline
        enhanced_timeline_with_static, visual_map = add_static_shots(
            enhanced_timeline,
            visual_map,
            webpage_video_path,
            logo_path,
            total_audio_duration,
            opening_duration=3.0,
            closing_duration=3.0
        )
        
        # Update the enhanced timeline
        enhanced_timeline = enhanced_timeline_with_static

        # Final data structure
        result = {
            'visual_timeline': enhanced_timeline,
            'visual_map': visual_map,
            'segments': enhanced_timeline
        }

        # Save the AI-generated visual map
        visual_map_ai_path = os.path.join(run_dir, 'visual_map_ai.json')
        with open(visual_map_ai_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2)
        
        # Also save as visual_map.json for component compatibility
        visual_map_path = os.path.join(run_dir, 'visual_map.json')
        with open(visual_map_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2)
        
        logging.info(f"‚úÖ AI Visual Director completed with {len(enhanced_timeline)} visuals.")
        return result

    except Exception as e:
        logging.error(f"‚ùå AI Visual Director failed: {e}", exc_info=True)
        logger.log_decision(
            step="ai_visual_director_error",
            decision="AI visual director failed",
            reasoning=f"Error: {e}",
            confidence=0.0
        )
        return None
